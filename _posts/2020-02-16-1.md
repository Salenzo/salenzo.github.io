Python3爬虫实战
==================================================================

TODO\_LIST
-------------------------------------------------

-   Request对象
-   selenium
-   日志系统缺陷完善
-   进程/线程管理
-   数据解析
-   数据清洗
-   Scrapy框架
-   模拟登录

***CODE*** :
-----------------------------------------------------

```python
import datetime
from bs4 import BeautifulSoup as bs
from requests import Session
from time import sleep
from urllib import request
import os

with request.urlopen('http://metsubojinrai.top/') as file_web:
    # 储存日志
    data = file_web.read()
    with open('./webcrawler/test_webcrawler/log.txt', 'a') as file1:
        file1.write('Status: {} {} \n'.format(
            file_web.status, file_web.reason))
        for k, v in file_web.getheaders():
            file1.write('%s: %s\n' % (k, v))
        file1.write('\nTime : {}\n'.format(datetime.datetime.now()))
    # 储存网页源代码
    with open('./webcrawler/test_webcrawler/data.txt', 'w', encoding='utf-8') as file2:
        html = data.decode('utf-8')
        file2.write('{}'.format(html))
        file2.write('\nTime : {}'.format(datetime.datetime.now()))
```

-   **著者：** akhia
-   **記事へのリンク：**
    <http://metsubojinrai.top/2020/02/16/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/>
-   **著作権表示：** このブログ内のすべての記事は、特別な記載がない限り
    [BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/zh-CN)
    の下のライセンスで保護されています。

![akhia](https://avatars1.githubusercontent.com/u/50533365?s=400&u=6af17f99706e4462bd709f69dad33891a481a784&v=4)
